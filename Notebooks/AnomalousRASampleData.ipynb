{"cells":[{"cell_type":"markdown","source":["This notebook demonstrates the use of Anomalous Resource Access model in Sentinel.  It generates training and testing data, trains the Anomalous Resource Access model and uses it to score the test data.  The top predicted scores are submitted to Sentinel workspace.\n\nSteps:\n   0. One-time: Install the following packages on the cluster (refer: https://docs.databricks.com/libraries.html#install-a-library-on-a-cluster)\n        - sentinel_ai....whl package\n        - azure_sentinel_ml_utilities whl package\n        - scikit-surprise==1.0.6 (from PyPi)\n        - numpy==1.15.0 (from PyPi)\n        - pyarrow==0.12.0 (from PyPi)\n        - plotly (from PyPi)\n        \n   1. One-time: Set credentials in KeyVault so the notebook can access \n        - Log Analytics\n\n Storing and retrieving secrets: \n    - Using Azure KeyVault:- https://docs.azuredatabricks.net/security/secrets/secret-scopes.html"],"metadata":{}},{"cell_type":"markdown","source":["# Initialization"],"metadata":{}},{"cell_type":"code","source":["# Specify the Log Analytics (LA) WorkSpaceId (of your Sentinel instance).  The workspacekey should be kept in the KeyVault as the best security practice\n\n# Log Analytics WorkSpace Info found in the 'Advanced Settings' page of your LA workspace\nworkspace_id = 'YOUR LOG ANALYTICS WORKSPACE ID HERE'\n\n# For the shared key, use either the primary or the secondary key of the workspace. It is recommended that you store and get your key as a secret\nworkspace_shared_key = dbutils.secrets.get(scope = 'NAME HERE', key = 'KEY NAME HERE')"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\n\nfrom pyspark.sql import functions as f, types as t\nfrom pyspark.sql.window import *\n\n# ML\nfrom sentinel_ai.peer_anomaly import spark_collaborative_filtering as scf \n\n# spark\nfrom sentinel_ai.utils import sparkutils\n\n#utils\nfrom azure_sentinel_ml_utilities.log_analytics import log_analytics_client"],"metadata":{"ExecuteTime":{"end_time":"2019-02-11T12:27:03.600532Z","start_time":"2019-02-11T12:27:01.472150Z"},"collapsed":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# The following module generates random training and testing data sets. Two test data sets are generated, one with low anomaly and the other with high anomaly compared to the training\n# data. Note that the data generated has a 'score' field that is a seed value for training. When working with real data, you will have a timestamp that you will need to use to\n# calculate a score based on the aggregation of access over a time interval (hourly or daily).\ntrain, test_high_anomaly, test_low_anomaly = scf.ProfileAccessDataset.gen_large_cluster_data(\n                                                                                    25,\n                                                                                    100,\n                                                                                    10\n                                                                                )\n\ndataSchema = t.StructType([\n            t.StructField('orgUser', t.StringType()),\n            t.StructField('orgRes', t.StringType()),\n            t.StructField('score', t.FloatType())\n        ])\n\ntmp_train_df = spark.createDataFrame(train, schema=dataSchema)\ntmp_test_high_anomaly_df = spark.createDataFrame(test_high_anomaly, schema=dataSchema)\ntmp_test_low_anomaly_df = spark.createDataFrame(test_low_anomaly, schema=dataSchema)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# This module does 3 things:\n#  a) Makes the values of users and resources look more readable (that just numbers)\n#  b) Gives a default tenant ID of 0\n#  c) Adds a count for number of access (for displaying the graph)\ndef updateValues(df, use_random_count):\n  tmp_df = df.withColumn('user', f.concat(f.lit('user_'), f.col(\"orgUser\")))\n  tmp_df = tmp_df.withColumn('res', f.concat(f.lit('res_'), f.col(\"orgRes\"))).select(f.lit('0').alias('tid'), 'user', 'res', 'score')\n  if use_random_count:\n     return tmp_df.withColumn('count_', f.round(1+f.rand()*10))\n  else:\n     return tmp_df.withColumn('count_', f.lit(1))\n\n\ntrain_df = updateValues(tmp_train_df, True)\ntest_high_anomaly_df = updateValues(tmp_test_high_anomaly_df, False)\ntest_low_anomaly_df = updateValues(tmp_test_low_anomaly_df, False)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["display(train_df)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Stats of each dataframes\ncounts = train_df.agg(*(f.countDistinct(f.col(c)).alias(c) for c in train_df.columns))\ncounts.show()\n\ncounts = test_high_anomaly_df.agg(*(f.countDistinct(f.col(c)).alias(c) for c in test_high_anomaly_df.columns))\ncounts.show()\n\ncounts = test_low_anomaly_df.agg(*(f.countDistinct(f.col(c)).alias(c) for c in test_low_anomaly_df.columns))\ncounts.show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["# Meta params"],"metadata":{}},{"cell_type":"code","source":["train_df.describe().show()"],"metadata":{"ExecuteTime":{"end_time":"2019-02-11T12:33:33.947460Z","start_time":"2019-02-11T12:33:31.368658Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["# Training"],"metadata":{}},{"cell_type":"code","source":["access_anomaly = scf.AccessAnomaly(tenant_colname='tid', score_colname='score')\naccess_anomaly_model = access_anomaly.fit(train_df)"],"metadata":{"ExecuteTime":{"end_time":"2019-02-11T12:47:42.112994Z","start_time":"2019-02-11T12:40:47.542426Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["# Testing"],"metadata":{}},{"cell_type":"code","source":["# Score the high anomaly test dataset\npred_df = access_anomaly_model.transform(test_high_anomaly_df)"],"metadata":{"ExecuteTime":{"end_time":"2019-02-11T12:47:52.432190Z","start_time":"2019-02-11T12:47:42.116353Z"},"collapsed":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":["pred_df.first()"],"metadata":{"ExecuteTime":{"end_time":"2019-02-11T12:47:52.978362Z","start_time":"2019-02-11T12:47:52.435310Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":["pred_df.select('score', 'predicted_score').describe().show()"],"metadata":{"ExecuteTime":{"end_time":"2019-02-11T12:47:54.365653Z","start_time":"2019-02-11T12:47:52.981448Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### Report results"],"metadata":{}},{"cell_type":"code","source":["full_res_df = pred_df.orderBy(f.desc('predicted_score')).cache()"],"metadata":{"ExecuteTime":{"end_time":"2019-02-11T12:48:01.733431Z","start_time":"2019-02-11T12:48:01.723119Z"},"collapsed":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":["display(full_res_df)"],"metadata":{"ExecuteTime":{"end_time":"2019-02-11T12:48:02.547892Z","start_time":"2019-02-11T12:48:01.736469Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["#Rank anomalous users"],"metadata":{}},{"cell_type":"code","source":["#\n# Select a subset of results to send to Log Analytics\n#\nfrom pyspark.sql.window import Window\n\nw = Window.partitionBy(\n                  'tid',\n                  'res',\n                  'user'\n                ).orderBy(\n                  f.desc('predicted_score')\n                )\n\n# select values above threshold\nresults_above_threshold = full_res_df.filter(full_res_df.predicted_score > 1.0)\n\n# get distinct resource/user and corresponding timestamp and highest score\nresults_to_la = results_above_threshold.withColumn(\n                  'index', f.row_number().over(w)\n                  ).orderBy(\n                    f.desc('predicted_score')\n                  ).select(\n                    'tid',\n                    f.col('res').alias('Resource'),\n                    f.col('user').alias('Actor'),\n                    'predicted_score'\n                  ).where(\n                    'index == 1'\n                  ).limit(100)\n  \ndisplay(results_to_la)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["def print_ratio(df, thr):\n    print('ratio of above {0} items {1}/{2} = {3}%'.format(\n        thr,\n        df.filter(f.col('predicted_score') > thr).count(),\n        df.count(),\n        100.0*df.filter(f.col('predicted_score') > thr).count()/df.count()\n    ))\n    \nprint_ratio(full_res_df, 0)\nprint_ratio(full_res_df, 1.0)\nprint_ratio(full_res_df, 2.0)\nprint_ratio(full_res_df, 3.0)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["#Display all resource accesses by users with highest anomalous score"],"metadata":{}},{"cell_type":"code","source":["from plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot, offline\nprint (__version__) # requires version >= 1.9.0\n\n# run plotly in offline mode\noffline.init_notebook_mode()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["#Find all server accesses of users with high predicted scores\n# For display, limit to top 25 results\nresults_to_display = results_to_la.orderBy(\n                  f.desc('predicted_score')\n                ).limit(25)\ninteresting_users = train_df.join(results_to_display, f.col('user') == f.col('Actor'), \"inner\")\nresource_accesses_in_train = interesting_users.groupBy(\n                          'user', \n                          'res'\n                        ).agg(\n                          f.sum('count_').alias('count'),\n                        ).select(\n                          f.col('user'),\n                          f.col('res'),\n                          'count'\n                        )\n\n# pick top 5 resource accessed per user\nw = Window.partitionBy(\n                  'user'\n                ).orderBy(\n                  f.desc('count')\n                )\n\n# get distinct resource/user and corresponding timestamp and highest score\ntop_resource_accesses_in_train = resource_accesses_in_train.withColumn(\n                  'index', f.row_number().over(w)\n                  ).orderBy(\n                    f.desc('count')\n                  ).select(\n                    f.col('user').alias('Actor'),\n                    f.col('res').alias('Resource'),\n                    'count'\n                  ).where(\n                    f.col('index').isin([1,2,3,4,5])\n                  ).limit(100)\n\n# add the resource access with high score\nall_resource_access_df = top_resource_accesses_in_train.union(test_high_anomaly_df.select(f.col('user').alias('Actor'), f.col('res').alias('Resource'), f.col('count_').alias('count')))\n\n# get unique users and file shares\nhigh_scores_df = all_resource_access_df.toPandas()\nunique_arr = np.append(high_scores_df.Actor.unique(), high_scores_df.Resource.unique())\n\nunique_df = pd.DataFrame(data = unique_arr, columns = ['name'])\nunique_df['index'] = range(0, len(unique_df.index))\n\n# create index for source & target and color for the normal accesses\nnormal_line_color = 'rgba(211, 211, 211, 0.8)'\nanomolous_color = 'red'\nx = pd.merge(high_scores_df, unique_df, how='left', left_on='Actor', right_on='name').drop(['name'], axis=1).rename(columns={'index' : 'ActorIndex'})\nall_access_index_df = pd.merge(x, unique_df, how='left', left_on='Resource', right_on='name').drop(['name'], axis=1).rename(columns={'index' : 'ResourceIndex'})\nall_access_index_df['color'] = normal_line_color\n\n# results_to_display index, color and \ny = results_to_display.toPandas().drop(['tid', 'predicted_score'], axis=1)\ny = pd.merge(y, unique_df, how='left', left_on='Actor', right_on='name').drop(['name'], axis=1).rename(columns={'index' : 'ActorIndex'})\nhigh_scores_index_df = pd.merge(y, unique_df, how='left', left_on='Resource', right_on='name').drop(['name'], axis=1).rename(columns={'index' : 'ResourceIndex'})\nhigh_scores_index_df['count'] = 1\nhigh_scores_index_df['color'] = anomolous_color\n\n# combine the two tables\nframes = [all_access_index_df, high_scores_index_df]\ndisplay_df = pd.concat(frames)\n# display_df.head()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["results_to_la.count()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["data_trace = dict(\n    type='sankey',\n    domain = dict(\n      x =  [0,1],\n      y =  [0,1]\n    ),\n    orientation = \"h\",\n    valueformat = \".0f\",\n    node = dict(\n      pad = 10,\n      thickness = 30,\n      line = dict(\n        color = \"black\",\n        width = 0\n      ),\n      label = unique_df['name'].dropna(axis=0, how='any')\n    ),\n    link = dict(\n      source = display_df['ActorIndex'].dropna(axis=0, how='any'),\n      target = display_df['ResourceIndex'].dropna(axis=0, how='any'),\n      value = display_df['count'].dropna(axis=0, how='any'),\n      color = display_df['color'].dropna(axis=0, how='any'),\n  )\n)\n\nlayout =  dict(\n    title = \"All resources accessed by users with highest anomalous scores\",\n    height = 772,\n    font = dict(\n      size = 10\n    ),    \n)\n\nfig = dict(data=[data_trace], layout=layout)\n\np = plot(fig, output_type='div')\n\ndisplayHTML(p)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["def send_results_to_log_analytics(df_to_la):\n  # The log type is the name of the event that is being submitted.  This will show up under \"Custom Logs\" as log_type + '_CL'\n  log_type = 'AnomalousResourceAccessResult'\n\n  # concatenate columns to form one json record\n  json_records = df_to_la.withColumn('json_field', f.concat(f.lit('{'), \n                                            f.lit(' \\\"TimeStamp\\\": \\\"2019-11-01 12:00:00'), f.lit('\\\",'),\n                                            f.lit(' \\\"Actor\\\": \\\"'), f.col('Actor'), f.lit('\\\",'),\n                                            f.lit(' \\\"Resource\\\": \\\"'), f.col('Resource'), f.lit('\\\",'),\n                                            f.lit(' \\\"PredictedScore\\\":'), f.col('predicted_score'),\n                                            f.lit('}')\n                                           )                       \n                                         )\n  # combine json record column to create the array\n  json_body = json_records.agg(f.concat_ws(\", \", f.collect_list('json_field')).alias('body'))\n\n  if len(json_body.first()) > 0:\n    json_payload = json_body.first()['body']\n    json_payload = '[' + json_payload + ']'\n\n    payload = json_payload.encode('utf-8') #json.dumps(json_payload)\n    #print(payload)\n    return log_analytics_client(workspace_id, workspace_shared_key).post_data(payload, log_type)\n  else:\n    return \"No json data to send to LA\"\n\ncount = results_to_la.count()\nif count > 0:\n  print ('Results count = ', count)\n  result = send_results_to_log_analytics(results_to_la)\n  print(\"Writing to Log Analytics result: \", result)\nelse:\n  print ('No results to send to LA')"],"metadata":{},"outputs":[],"execution_count":28}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.2","nbconvert_exporter":"python","file_extension":".py"},"name":"AnomalousRASampleData","notebookId":1477580580719818,"kernelspec":{"display_name":"Python [default]","language":"python","name":"python3"},"anaconda-cloud":{},"toc":{"title_sidebar":"Contents","nav_menu":{},"sideBar":true,"number_sections":true,"skip_h1_title":false,"base_numbering":1,"toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false,"title_cell":"Table of Contents"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":0}
