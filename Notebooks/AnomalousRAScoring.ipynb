{"cells":[{"cell_type":"markdown","source":["This notebook is used for scoring using anomalous resource access model. The model was saved by the training Notebook.  This Notebook runs on a schedule, loads the model and score new events. The data used here is File Share Access Events from Windows machine. Data is loaded from a Blob Storage Container. The top scored results are submitted to Log Analytics.\n\nSteps:\n   0. One-time: Install the following packages on the cluster (refer: https://docs.databricks.com/libraries.html#install-a-library-on-a-cluster)\n        - sentinel_ai....whl package\n        - azure_sentinel_ml_utilities whl package\n        - azure-storage-blob==2.1.0 (from PyPi)\n        - scikit-surprise==1.0.6 (from PyPi)\n        - numpy==1.15.0 (from PyPi)\n        - pyarrow==0.12.0 (from PyPi)\n        - plotly (from PyPi)\n        \n   1. One-time: Set credentials in KeyVault so the notebook can access \n        - Storage Account\n        - Log Analytics\n   2. Ensure the relative paths to Blob Storage are correct.\n   3. Set the Notebook to run on a schedule to score and submit results to LA.\n   \n One-time: (Setting up Storage Key & Log Analytics Key in KeyVault)\n    - (Refer:- https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html#access-azure-blob-storage-directly)\n     \n Storing and retrieving secrets: \n    - Using Azure KeyVault:- https://docs.azuredatabricks.net/security/secrets/secret-scopes.html"],"metadata":{}},{"cell_type":"code","source":["import datetime as dt\n\n# Storage Account information\nstorage_account = 'YOUR STORAGE ACCOUNT HERE'\nstorage_key = dbutils.secrets.get(scope = 'NAME HERE', key = 'KEY NAME HERE')\ncontainer = 'CONTAINER NAME HERE'\nmount_point_name = 'MOUNT POINT NAME HERE'\n\ntest_base_path = 'PATHNAME HERE'\n\n# Log Analytics WorkSpace Info (ASI)\nworkspace_id = 'YOUR LOG ANALYTICS WORKSPACE ID HERE'\n# For the shared key, use either the primary or the secondary key of the workspace\nworkspace_shared_key = dbutils.secrets.get(scope = 'NAME HERE', key = 'KEY NAME HERE')\n# Project name\nproject = 'PROJECT NAME HERE'\n\n###\n### Note that when scheduling periodically, you specify time range relative to current time as specified in the commented section below\n###\n# Time range for testing\n# test_start_time = dt.datetime.now() - dt.timedelta(hours=1)\n# test_end_time = dt.datetime.now()\n\ntest_start_time = dt.datetime.strptime('Jan 20 2019', '%b %d %Y') \ntest_end_time = dt.datetime.strptime('Jan 24 2019', '%b %d %Y') \n\nmodel_path = '{root}/{project}/model_output'.format(root=mount_point_name + 'models/', project=project)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["###\n### You can do this one-time in a separate Notebook, so that you don't cause accidental errors in other Notebooks mounting/unmounting the folder\n###\n\n# Mount the Storage Container\n#    (Refer:- https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html#mount-azure-blob-storage-containers-with-dbfs)\ndbutils.fs.mount(\n  source = \"wasbs://\" + container + \"@\" + storage_account + \".blob.core.windows.net\",\n  mount_point = mount_point_name,\n  extra_configs = {\"fs.azure.account.key.\" + storage_account + \".blob.core.windows.net\":storage_key})"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\n\nfrom pyspark.sql import functions as f, types as t\nfrom pyspark.sql.functions import udf\n\n# ML\nfrom sentinel_ai.peer_anomaly import spark_collaborative_filtering as scf \n\n# spark\nfrom sentinel_ai.utils import sparkutils\n\n#utils\nfrom azure_sentinel_ml_utilities.azure_storage import blob_manager\nfrom azure_sentinel_ml_utilities.log_analytics import log_analytics_client"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["#Load saved model"],"metadata":{}},{"cell_type":"code","source":["access_anomaly_model = scf.AccessAnomalyModel.load(\n    spark, \n    '{model_path}/access_anomaly_model'.format(model_path=model_path)\n)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["# Dataset"],"metadata":{}},{"cell_type":"code","source":["class FileShareDataset:\n  \n    def __init__(self, storage_account, storage_key):\n      self.storage_account = storage_account\n      self.blob_manager = blob_manager(storage_account, storage_key)\n      # Spark conf set for spark.read.csv to work\n      spark.conf.set(\n        \"fs.azure.account.key.\" + storage_account + \".blob.core.windows.net\",\n        storage_key)\n\n    @staticmethod\n    def get_schema():\n      return t.StructType([\n        t.StructField('Timestamp', t.TimestampType()),\n        t.StructField('Actor', t.StringType()),\n        t.StructField('Resource', t.StringType()),\n        t.StructField('categoricalFeatures', t.StringType()),\n        t.StructField('count_', t.IntegerType())\n      ])\n\n    def get_raw_df(self, start_time, end_time, container, root, use_schema=True):       \n      blob_names = self.blob_manager.enumerate_blob_names(start_time, end_time, container, root)\n      full_blob_names = [\"wasbs://\" + container + \"@\" + self.storage_account + \".blob.core.windows.net/\" + bn for bn in blob_names]\n      \n      schema = FileShareDataset.get_schema() if use_schema else None\n      \n      if use_schema:\n          return spark.read.csv(full_blob_names, schema=schema, sep='\\t', header=False)\n      else:\n          return spark.read.csv(full_blob_names, sep='\\t', header=False)\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["def getdataset():\n  fileShareDataset = FileShareDataset(storage_account, storage_key)\n  return (\n    fileShareDataset.get_raw_df(test_start_time, test_end_time, container, test_base_path).select(\n      f.lit('0').alias('tid'),\n      f.col('Timestamp').alias('timestamp'),\n      f.col('Actor').alias('user'),\n      f.col('Resource').alias('res'),\n      f.col('categoricalFeatures').alias('categorical_features')\n    ).cache()\n  )"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["ptesting = getdataset()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["print(ptesting.first())"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["ptesting.describe().show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["# Scoring"],"metadata":{}},{"cell_type":"code","source":["pred_df = access_anomaly_model.transform(ptesting)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["pred_df.first()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["pred_df.select('predicted_score').describe().show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# report results"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["full_res_df = pred_df.orderBy(f.desc('predicted_score'))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["full_res_df.first()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# Check score of a simulated anomolous user access\n\n#anomalous_user_access = full_res_df.filter(full_res_df.user.like('Domain_282/User_871048'))\n#display(anomalous_user_access)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["##Filter out commonly seen users (automation account that are known to access File Shares)"],"metadata":{}},{"cell_type":"code","source":["# If there are automation user accounts that access different shares and can cause false positives then filter such users out\nusersToFilter = ['Domain_346/User_870818', 'Domain_348/User_231659']\nfiltered_result = full_res_df.filter(full_res_df.user.isin(*usersToFilter) == False)\nfiltered_result = filtered_result.where(f.col('user').endswith('User_255625') == False) # automation user in all domains\nprint(full_res_df.count())\nprint(filtered_result.count())"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["##Rank top anomalous users"],"metadata":{}},{"cell_type":"code","source":["#\n# Select a subset of results to send to Log Analytics\n#\nfrom pyspark.sql.window import Window\n\nw = Window.partitionBy(\n                  'tid',\n                  'res',\n                  'user'\n                ).orderBy(\n                  f.desc('predicted_score')\n                )\n\n# select values above threshold\nresults_above_threshold = filtered_result.filter(filtered_result.predicted_score > 7.75)\n\n# get distinct resource/user and corresponding timestamp and highest score\nresults_to_la = results_above_threshold.withColumn(\n                  'index', f.row_number().over(w)\n                  ).orderBy(\n                    f.desc('predicted_score')\n                  ).select(\n                    'tid',\n                    f.col('res').alias('Resource'),\n                    f.col('user').alias('Actor'),\n                    'categorical_features',\n                    'timestamp',\n                    'predicted_score'\n                  ).where(\n                    'index == 1'\n                  ).limit(25)\n  \ndisplay(results_to_la)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["#Write top anomalous scores to Sentinel"],"metadata":{}},{"cell_type":"code","source":["def send_results_to_log_analytics(df_to_la):\n  # The log type is the name of the event that is being submitted.  This will show up under \"Custom Logs\" as log_type + '_CL'\n  log_type = 'AnomalousResourceAccessResult'\n\n  # concatenate columns to form one json record\n  json_records = df_to_la.withColumn('json_field', f.concat(f.lit('{'), \n                                            f.lit(' \\\"TimeStamp\\\": \\\"'), f.from_unixtime(f.unix_timestamp(f.col(\"timestamp\")), \"y-MM-dd'T'hh:mm:ss.SSS'Z'\"), f.lit('\\\",'),\n                                            f.lit(' \\\"Actor\\\": \\\"'), f.col('Actor'), f.lit('\\\",'),\n                                            f.lit(' \\\"Resource\\\": \\\"'), f.col('Resource'), f.lit('\\\",'),\n                                            f.lit(' \\\"PredictedScore\\\":'), f.col('predicted_score'),\n                                            f.lit('}')\n                                           )                       \n                                         )\n  # combine json record column to create the array\n  json_body = json_records.agg(f.concat_ws(\", \", f.collect_list('json_field')).alias('body'))\n\n  if len(json_body.first()) > 0:\n    json_payload = json_body.first()['body']\n    json_payload = '[' + json_payload + ']'\n\n    payload = json_payload.encode('utf-8') #json.dumps(json_payload)\n    # print(payload)\n    return log_analytics_client(workspace_id, workspace_shared_key).post_data(payload, log_type)\n  else:\n    return \"No json data to send to LA\"\n\ncount = results_to_la.count()\nif count > 0:\n  print ('Results count = ', count)\n  result = send_results_to_log_analytics(results_to_la)\n  print(\"Writing to Log Analytics result: \", result)\nelse:\n  print ('No results to send to LA')"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# users that were not in the training set\nnever_seen_users = full_res_df.where(f.col('predicted_score').isNull()).select(f.col('user')).distinct()\n\nprint('Count never seen users:', never_seen_users.count())\ndisplay(never_seen_users)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["def print_ratio(df, thr):\n    print('ratio of above {0} items {1}/{2} = {3}%'.format(\n        thr,\n        df.filter(f.col('predicted_score') > thr).count(),\n        df.count(),\n        100.0*df.filter(f.col('predicted_score') > thr).count()/df.count()\n    ))\n    \nprint_ratio(full_res_df, 0)\nprint_ratio(full_res_df, 2.5)\nprint_ratio(full_res_df, 5)\nprint_ratio(full_res_df, 7.5)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["#Display all resource accesses by users with highest anomalous score"],"metadata":{}},{"cell_type":"code","source":["from plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot, offline\nprint (__version__) # requires version >= 1.9.0\n\n# run plotly in offline mode\noffline.init_notebook_mode()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["#Find all server accesses of users with high predicted scores\n# For display, limit to top 25 results\nresults_to_display = results_to_la.orderBy(\n                  f.desc('predicted_score')\n                ).limit(15)\ninteresting_users = filtered_result.join(results_to_display, f.col('user') == f.col('Actor'), \"inner\")\nfileShare_accesses = interesting_users.groupBy(\n                          'user', \n                          'res'\n                        ).agg(\n                          f.count('*').alias('count'),\n                        ).select(\n                          f.col('user').alias('Actor'),\n                          f.col('res').alias('Resource'),\n                          'count'\n                        )\n\n# get unique users and file shares\nhigh_scores_df = fileShare_accesses.toPandas()\nunique_arr = np.append(high_scores_df.Actor.unique(), high_scores_df.Resource.unique())\n\nunique_df = pd.DataFrame(data = unique_arr, columns = ['name'])\nunique_df['index'] = range(0, len(unique_df.index))\n\n# create index for source & target and color for the normal accesses\nnormal_line_color = 'rgba(211, 211, 211, 0.8)'\nanomolous_color = 'red'\nx = pd.merge(high_scores_df, unique_df, how='left', left_on='Actor', right_on='name').drop(['name'], axis=1).rename(columns={'index' : 'ActorIndex'})\nall_access_index_df = pd.merge(x, unique_df, how='left', left_on='Resource', right_on='name').drop(['name'], axis=1).rename(columns={'index' : 'ResourceIndex'})\nall_access_index_df['color'] = normal_line_color\n\n# results_to_display index, color and \ny = results_to_display.toPandas().drop(['tid', 'categorical_features', 'timestamp', 'predicted_score'], axis=1)\ny = pd.merge(y, unique_df, how='left', left_on='Actor', right_on='name').drop(['name'], axis=1).rename(columns={'index' : 'ActorIndex'})\nhigh_scores_index_df = pd.merge(y, unique_df, how='left', left_on='Resource', right_on='name').drop(['name'], axis=1).rename(columns={'index' : 'ResourceIndex'})\nhigh_scores_index_df['count'] = 1\nhigh_scores_index_df['color'] = anomolous_color\n\n# substract 1 for the red entries in all_access df\nhsi_df = high_scores_index_df[['Actor','Resource', 'count']].rename(columns={'count' : 'hsiCount'})\nall_access_updated_count_df = pd.merge(all_access_index_df, hsi_df, how='left', left_on=['Actor', 'Resource'], right_on=['Actor', 'Resource'])\nall_access_updated_count_df['count'] = np.where(all_access_updated_count_df['hsiCount']==1, all_access_updated_count_df['count'] - 1, all_access_updated_count_df['count'])\nall_access_updated_count_df = all_access_updated_count_df.loc[all_access_updated_count_df['count'] > 0]\nall_access_updated_count_df = all_access_updated_count_df[['Actor','Resource', 'count', 'ActorIndex', 'ResourceIndex', 'color']]\n\n# combine the two tables\nframes = [all_access_updated_count_df, high_scores_index_df]\ndisplay_df = pd.concat(frames)\n# display_df.head()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["data_trace = dict(\n    type='sankey',\n    domain = dict(\n      x =  [0,1],\n      y =  [0,1]\n    ),\n    orientation = \"h\",\n    valueformat = \".0f\",\n    node = dict(\n      pad = 10,\n      thickness = 30,\n      line = dict(\n        color = \"black\",\n        width = 0\n      ),\n      label = unique_df['name'].dropna(axis=0, how='any')\n    ),\n    link = dict(\n      source = display_df['ActorIndex'].dropna(axis=0, how='any'),\n      target = display_df['ResourceIndex'].dropna(axis=0, how='any'),\n      value = display_df['count'].dropna(axis=0, how='any'),\n      color = display_df['color'].dropna(axis=0, how='any'),\n  )\n)\n\nlayout =  dict(\n    title = \"All resources accessed by users with highest anomalous scores\",\n    height = 772,\n    font = dict(\n      size = 10\n    ),    \n)\n\nfig = dict(data=[data_trace], layout=layout)\n\np = plot(fig, output_type='div')\n\ndisplayHTML(p)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# unmount blob storage\ndbutils.fs.unmount(mount_point_name)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":34}],"metadata":{"name":"AnomalousRAScoring","notebookId":1477580580719955},"nbformat":4,"nbformat_minor":0}
