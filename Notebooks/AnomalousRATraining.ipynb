{"cells":[{"cell_type":"markdown","source":["This notebook is used for training anomalous resource access model.  The data used here is File Share Access Events from Windows machine. Data is loaded from a Blob Storage Container.\nThe trained model is then saved to the Blob Storage, which can then be used by the Scoring Notebook\n\nSteps:\n   0. One-time: Install the following packages on the cluster (refer: https://docs.databricks.com/libraries.html#install-a-library-on-a-cluster)\n        - sentinel_ai....whl package\n        - azure_sentinel_ml_utilities whl package\n        - azure-storage-blob==2.1.0 (from PyPi)\n        - scikit-surprise==1.0.6 (from PyPi)\n        - numpy==1.15.0 (from PyPi)\n        - pyarrow==0.12.0 (from PyPi)\n        \n   1. One-time: Set credentials in KeyVault so the notebook can access \n        - Storage Account\n   2. Ensure the relative paths to Blob Storage are correct.\n   3. Run the Notebook to produce the model\n   \n One-time: (Setting up Storage Key in KeyVault)\n    - (Refer:- https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html#access-azure-blob-storage-directly)\n \n Storing and retrieving secrets: \n    - Using Azure KeyVault:- https://docs.azuredatabricks.net/security/secrets/secret-scopes.html"],"metadata":{}},{"cell_type":"code","source":["import datetime as dt\n\n# Storage Account information\nstorage_account = 'YOUR STORAGE ACCOUNT HERE'\nstorage_key = dbutils.secrets.get(scope = 'NAME HERE', key = 'KEY NAME HERE')\ncontainer = 'CONTAINER NAME HERE'\nmount_point_name = 'MOUNT POINT NAME HERE'\n\ntrain_base_path = 'PATHNAME HERE'\n\n# Project name\nproject = 'PROJECT NAME HERE'\n\n###\n### Note that when training periodically, you specify time range relative to current time as specified in the commented section\n###\n# Time range for training\n# train_start_time = dt.datetime.now() - dt.timedelta(days=65)\n# train_end_time = dt.datetime.now() - dt.timedelta(days=10)\ntrain_start_time = dt.datetime.strptime('Dec 1 2018', '%b %d %Y') \ntrain_end_time = dt.datetime.strptime('Jan 20 2019', '%b %d %Y') "],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["###\n### You can do this one-time in a separate Notebook, so that you don't cause accidental errors in other Notebooks mounting/unmounting the folder\n###\n\n# Mount the Storage Container\n#    (Refer:- https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html#mount-azure-blob-storage-containers-with-dbfs)\ndbutils.fs.mount(\n source = \"wasbs://\" + container + \"@\" + storage_account + \".blob.core.windows.net\",\n mount_point = mount_point_name,\n extra_configs = {\"fs.azure.account.key.\" + storage_account + \".blob.core.windows.net\":storage_key})"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\n\nfrom pyspark.sql import functions as f, types as t\nfrom pyspark.sql.functions import udf\n\n# ML\nfrom sentinel_ai.peer_anomaly import spark_collaborative_filtering as scf \n\n# spark\nfrom sentinel_ai.utils import sparkutils\n\n#utils\nfrom azure_sentinel_ml_utilities.azure_storage import blob_manager"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["checkpoint_dir = mount_point_name + 'cache/{0}/checkpoints'.format(project)\ndbutils.fs.mkdirs(checkpoint_dir)\nsparkutils.set_checkpointdir(spark, checkpoint_dir)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["# Load Dataset"],"metadata":{}},{"cell_type":"code","source":["class FileShareDataset:\n  \n    def __init__(self, storage_account, storage_key):\n      self.storage_account = storage_account\n      self.blob_manager = blob_manager(storage_account, storage_key)\n      # Spark conf set for spark.read.csv to work\n      spark.conf.set(\n        \"fs.azure.account.key.\" + storage_account + \".blob.core.windows.net\",\n        storage_key)\n\n    @staticmethod\n    def get_schema():\n      return t.StructType([\n        t.StructField('Timestamp', t.TimestampType()),\n        t.StructField('Actor', t.StringType()),\n        t.StructField('Resource', t.StringType()),\n        t.StructField('categoricalFeatures', t.StringType()),\n        t.StructField('count_', t.IntegerType())\n      ])\n\n    @staticmethod\n    def _make_days_delta():\n      @udf('double')\n      def days_delta(d2, d1):\n        return 1.0 + (d2 - d1).days\n\n      return days_delta\n\n    def get_raw_df(self, start_time, end_time, container, root, use_schema=True):\n        \n      blob_names = self.blob_manager.enumerate_blob_names(start_time, end_time, container, root)\n      full_blob_names = [\"wasbs://\" + container + \"@\" + self.storage_account + \".blob.core.windows.net/\" + bn for bn in blob_names]\n      \n      schema = FileShareDataset.get_schema() if use_schema else None\n      \n      if use_schema:\n          return spark.read.csv(full_blob_names, schema=schema, sep='\\t', header=False)\n      else:\n          return spark.read.csv(full_blob_names, sep='\\t', header=False)\n\n    def processed_df(self, df):\n        dd = FileShareDataset._make_days_delta()\n\n        return df.select(\n            f.col('Timestamp').alias('timestamp1'),\n            f.col('Timestamp').alias('timestamp2'),\n            'Actor',\n            'Resource',\n            'count_'\n        ).groupBy(\n            'Actor',\n            'Resource'\n        ).agg({\n            'timestamp1': 'min',\n            'timestamp2': 'max',\n            'count_': 'sum'\n        }).select(\n            f.lit('0').alias('tid'),\n            f.col('min(timestamp1)').alias('min_timestamp'),\n            f.col('max(timestamp2)').alias('max_timestamp'),\n            f.col('Actor').alias('user'),\n            f.col('Resource').alias('res'),\n            (f.col('sum(count_)')/dd(f.col('max(timestamp2)'), f.col('min(timestamp1)'))).alias('score')\n        )\n\n    def get_dataset(self, start_time, end_time, container, root):\n        return self.processed_df(self.get_raw_df(start_time, end_time, container, root)).cache()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["def getdataset():\n  return FileShareDataset(storage_account, storage_key).get_dataset(train_start_time, train_end_time, container, train_base_path)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["ptraining = getdataset()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["print(ptraining.first())"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["print(ptraining.select('tid').distinct().count())"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["ptraining.describe().show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["sparkutils.df_stats(ptraining)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["# Build Model"],"metadata":{}},{"cell_type":"code","source":["# Model building\naccess_anomaly = scf.AccessAnomaly(tenant_colname='tid', score_colname='score')\naccess_anomaly_model = access_anomaly.fit(ptraining)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["# Save Model"],"metadata":{}},{"cell_type":"code","source":["model_output = '{root}/{project}/model_output'.format(root=mount_point_name + 'models/', project=project)\nprint(model_output)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["access_anomaly_model.save(\n    '{model_output}/access_anomaly_model'.format(model_output=model_output)\n)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# unmount blob storage\ndbutils.fs.unmount(mount_point_name)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":20}],"metadata":{"name":"AnomalousRATraining","notebookId":1477580580719934},"nbformat":4,"nbformat_minor":0}
